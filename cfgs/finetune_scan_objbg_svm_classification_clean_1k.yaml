optimizer : {
  type: AdamW,
  part: only_new,
  kwargs: {
  lr : 0.0005,
  weight_decay : 0.05
}}

scheduler: {
  type: CosLR,
  kwargs: {
    epochs: 300,
    initial_epochs : 10
}}

## For Non-transformer backbones, e.g., DGCNN, please change the point number to 1024, since the pre-training is conducted with 1024 points.

dataset : {
  train : { _base_: cfgs/dataset_configs/ScanObjectNN_objectbg.yaml,
            others: {subset: 'train', npoints: 1024, aug_type: ['clean']}},
  val : { _base_: cfgs/dataset_configs/ScanObjectNN_objectbg.yaml,
            others: {subset: 'test', npoints: 1024, aug_type: ['clean']}},
  test : { _base_: cfgs/dataset_configs/ScanObjectNN_objectbg.yaml,
            others: {subset: 'test', npoints: 1024, aug_type: ['clean']}}}
model : {
  NAME: DGCNN_feat,
  smoothloss: True,
  cls_dim: 15,
  ## for transformer only
  trans_dim: 384,
  depth: 12,
  drop_path_rate: 0.1,
  num_heads: 6,
  group_size: 32,
  num_group: 128,
  encoder_dims: 384,
}


npoints: 1024
total_bs : 16
step_per_update : 1
max_epoch : 300
grad_norm_clip : 10
