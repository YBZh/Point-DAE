optimizer : {
  type: AdamW,
  part: all,
  kwargs: {
  lr : 0.0005,  ## return to the default.
#  lr : 0.005, ## original lr : 0.0005. however, diff_lr is much worse than all. So we enlarge the lr by * 10. not work! bad results
  weight_decay : 0.05
}}
###### PointNetNoT on ObjBG.
## diff_lr, 0.0005: 57%.  Results with Diff_lr setting is worse than all setting.
## diff_lr, 0.005: 62%
## all    , 0.001: 79.9%
## all    , 0.0005: 80%
## all    , 0.0001: 79.2.

scheduler: {
  type: CosLR,
  kwargs: {
    epochs: 200,
    initial_epochs : 10
}}

dataset : {
  train : { _base_: cfgs/dataset_configs/ScanObjectNN_objectbg.yaml,
            others: {subset: 'train', npoints: 1024, aug_type: ['translate', 'scale']}},
  val : { _base_: cfgs/dataset_configs/ScanObjectNN_objectbg.yaml,
            others: {subset: 'test', npoints: 1024, aug_type: ['clean']}},
  test : { _base_: cfgs/dataset_configs/ScanObjectNN_objectbg.yaml,
            others: {subset: 'test', npoints: 1024, aug_type: ['clean']}}}
model : {
  NAME: PointNetv2,
  smoothloss: False,
#  trans_dim: 384,
#  depth: 12,
#  drop_path_rate: 0.1,
  cls_dim: 15,
#  num_heads: 6,
#  group_size: 32,
#  num_group: 128,
#  encoder_dims: 384,
}

npoints: 1024
total_bs : 32
step_per_update : 1
max_epoch : 200
grad_norm_clip : 10
